"""
ts_forecast_mc_dropout_fixed.py

Fully functional pipeline implementing:
- LSTM multi-step forecasting (multi-horizon)
- Monte Carlo Dropout for uncertainty (MC Dropout INFERENCE uses training=True)
- Interval metrics: coverage probability and Mean Interval Score (MIS)
- Proper ARIMA baseline implemented as rolling forecasts
- Reproducibility, modularity, and small safeguards

Dependencies:
  pip install numpy pandas matplotlib scikit-learn tensorflow statsmodels tqdm

Usage:
  python ts_forecast_mc_dropout_fixed.py
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
import tensorflow as tf
from tensorflow.keras import layers, models
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# ---------------------------
# Utility / helper functions
# ---------------------------

def set_seeds(seed=42):
    np.random.seed(seed)
    tf.random.set_seed(seed)

def load_default_series():
    """Load the Sunspots series from statsmodels (univariate)."""
    import statsmodels.api as sm
    data = sm.datasets.sunspots.load_pandas().data
    series = data['SUNACTIVITY'].astype(float)
    # annual series preserved as RangeIndex â€” fine for modelling
    series.index = pd.RangeIndex(start=0, stop=len(series), step=1)
    return series

def load_csv_series(csv_path, date_col=None, value_col=None):
    df = pd.read_csv(csv_path)
    if date_col and value_col:
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(date_col)
        s = pd.Series(df[value_col].values, index=df[date_col])
    else:
        numeric = df.select_dtypes(include=[np.number])
        if numeric.shape[1] == 0:
            raise ValueError("No numeric column found in CSV.")
        s = pd.Series(numeric.iloc[:, 0].values, index=pd.RangeIndex(len(numeric)))
    return s.astype(float)

def adf_test(series):
    """Return ADF test statistic and p-value."""
    res = adfuller(series.dropna())
    return {'adf_stat': res[0], 'pvalue': res[1]}

def make_sequences(values, input_len, output_len):
    """
    Create supervised sequences from 1D array `values`.
    Returns X: (n_samples, input_len, 1), y: (n_samples, output_len)
    """
    X, y = [], []
    for i in range(len(values) - input_len - output_len + 1):
        X.append(values[i:i+input_len])
        y.append(values[i+input_len:i+input_len+output_len])
    X = np.array(X)[:,:,None]
    y = np.array(y)
    return X, y

def build_lstm_mc(input_len, output_len, dropout_rate=0.2, units=64):
    """
    Build an LSTM model with dropout layers. Dropout layers are kept
    and will be used at inference by calling model(x, training=True).
    """
    inp = layers.Input(shape=(input_len, 1))
    x = layers.LSTM(units, return_sequences=False)(inp)
    x = layers.Dropout(dropout_rate)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout_rate)(x)
    out = layers.Dense(output_len, activation='linear')(x)
    model = models.Model(inp, out)
    model.compile(optimizer='adam', loss='mse')
    return model

def batch_iter_predict_mc(model, X, T=100, batch_size=64):
    """
    Compute T stochastic forward passes in batches, using model(X_batch, training=True)
    Returns array shape (T, n_samples, output_len)
    """
    n = X.shape[0]
    outputs = []
    # We'll iterate T times and produce predictions per pass in batches
    for t in range(T):
        preds_t = []
        for i in range(0, n, batch_size):
            Xb = X[i:i+batch_size]
            # Force dropout by setting training=True
            pred_b = model(Xb, training=True).numpy()
            preds_t.append(pred_b)
        preds_t = np.vstack(preds_t)  # shape (n, out)
        outputs.append(preds_t)
    outputs = np.stack(outputs, axis=0)  # (T, n, out)
    return outputs

def compute_interval_metrics(y_true, samples, alpha):
    """
    Compute coverage and Mean Interval Score (MIS) for central (1-alpha) prediction intervals.
    y_true shape: (n, out)
    samples shape: (T, n, out)
    Returns:
      coverage (scalar fraction across all points/horizons),
      MIS (mean across all points/horizons),
      lower (n,out), upper (n,out)
    """
    lower_q = alpha / 2.0
    upper_q = 1.0 - alpha / 2.0
    lower = np.quantile(samples, lower_q, axis=0)
    upper = np.quantile(samples, upper_q, axis=0)

    inside = ((y_true >= lower) & (y_true <= upper)).astype(float)
    coverage = inside.mean()

    # MIS per element
    U_minus_L = (upper - lower)
    below = np.maximum(0.0, lower - y_true)
    above = np.maximum(0.0, y_true - upper)
    mis = U_minus_L + (2.0 / alpha) * (below + above)
    mis_mean = mis.mean()

    return coverage, mis_mean, lower, upper

def evaluate_point_metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))
    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())
    return rmse, mae

# ---------------------------
# ARIMA rolling baseline
# ---------------------------

def arima_rolling_forecast(series_values, train_cutoff_index, seq_start_positions, input_len, output_len, order=(5,1,0)):
    """
    Produce ARIMA baseline forecasts for each test sequence start in seq_start_positions.
    - series_values: full original 1D numpy array of the series (unscaled)
    - train_cutoff_index: index (in original series) of the last observation included in the training set
    - seq_start_positions: list/array of indices corresponding to sequence-window starts in the original series
      (i.e., these map to positions where we want to produce forecasts for the following output_len steps)
    - Returns array arima_preds shape (n_test, output_len)
    NOTE: This function fits ARIMA for each forecast start with history up to that start.
    """
    arima_forecasts = []
    for start_pos in seq_start_positions:
        # history_end is the index of the last observed point available to the forecaster
        history_end = train_cutoff_index + start_pos  # because seq_start_positions are relative to first test window
        # Bound the index
        history_end = int(history_end)
        # Fit ARIMA on series_values[:history_end+1]
        hist = series_values[:history_end+1]  # inclusive
        # If history length too short for ARIMA, fallback to naive repeat last
        if len(hist) < 10:
            # simple baseline: repeat last value
            f = np.repeat(hist[-1], output_len)
            arima_forecasts.append(f)
            continue
        try:
            model = ARIMA(hist, order=order).fit()
            f = model.forecast(steps=output_len)
            arima_forecasts.append(np.asarray(f))
        except Exception:
            # fallback to last-value baseline
            f = np.repeat(hist[-1], output_len)
            arima_forecasts.append(f)
    return np.vstack(arima_forecasts)

# ---------------------------
# Main pipeline
# ---------------------------

def run_pipeline(
    series,
    input_len=36,
    output_len=12,
    test_fraction=0.2,
    val_fraction=0.1,
    units=64,
    dropout_rate=0.2,
    batch_size=32,
    epochs=100,
    mc_T=200,
    random_seed=42,
    arima_order=(5,1,0),
    save_model_path="lstm_mc_dropout_model"
):
    set_seeds(random_seed)

    s = series.dropna().astype(float)
    if len(s) < (input_len + output_len + 10):
        raise ValueError("Series too short for chosen input/output lengths.")
    print("Series length:", len(s))
    adf = adf_test(s)
    print("ADF p-value:", adf['pvalue'])

    # Scaling for NN
    scaler = StandardScaler()
    values_scaled = scaler.fit_transform(s.values.reshape(-1,1)).flatten()

    # Create sequences
    X, y = make_sequences(values_scaled, input_len, output_len)
    n_samples = X.shape[0]

    test_size = int(n_samples * test_fraction)
    val_size = int(n_samples * val_fraction)
    train_end = n_samples - test_size - val_size  # index in sequence-space where train ends

    if train_end <= 0:
        raise ValueError("Not enough data for the requested splits (increase series length or reduce input/output sizes).")

    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:train_end+val_size], y[train_end:train_end+val_size]
    X_test, y_test = X[train_end+val_size:], y[train_end+val_size:]

    print("Train/Val/Test samples:", X_train.shape[0], X_val.shape[0], X_test.shape[0])

    # Build model
    model = build_lstm_mc(input_len, output_len, dropout_rate=dropout_rate, units=units)
    model.summary()

    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=2
    )

    # Deterministic point forecast (using model in default mode)
    y_pred_scaled = model.predict(X_test, batch_size=batch_size)
    # inverse scaling
    y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1,1)).reshape(y_test.shape)
    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).reshape(y_pred_scaled.shape)

    rmse, mae = evaluate_point_metrics(y_test_unscaled, y_pred_unscaled)
    print(f"LSTM point metrics: RMSE={rmse:.4f}, MAE={mae:.4f}")

    # MC Dropout sampling (force dropout on at inference by calling model(X, training=True))
    print(f"Running MC Dropout inference with T={mc_T} ... (this may take time)")
    samples_scaled = batch_iter_predict_mc(model, X_test, T=mc_T, batch_size=batch_size)  # (T, n, out)
    # invert scaling
    T, n_test, out = samples_scaled.shape
    samples_flat = samples_scaled.reshape(-1,1)
    samples_unscaled = scaler.inverse_transform(samples_flat).reshape(T, n_test, out)

    # Compute interval metrics for 80% and 95%
    interval_results = {}
    for alpha in [0.2, 0.05]:
        cov, mis, lower, upper = compute_interval_metrics(y_test_unscaled, samples_unscaled, alpha)
        interval_results[f"{int((1-alpha)*100)}%"] = {'coverage': float(cov), 'MIS': float(mis), 'lower': lower, 'upper': upper}
        print(f"{int((1-alpha)*100)}% interval: coverage={cov:.3f}, MIS={mis:.4f}")

    # ARIMA rolling baseline
    # Map sequence-space test windows to original series indices:
    # The i-th sequence (in sequence indexing) uses original data positions [i, i+input_len-1] as input,
    # and forecasts for positions [i+input_len, ..., i+input_len+output_len-1].
    seq_indices = np.arange(len(values_scaled) - input_len - output_len + 1)
    test_seq_indices = seq_indices[train_end+val_size:]  # start positions (in original index space)
    # For rolling ARIMA, we will supply actual historical values up to the start of each forecast.
    series_values_unscaled = s.values  # original unscaled data
    # train_cutoff is the last index of the original series used to fit the initial training model:
    # training used sequences up to train_end -> the last training sequence ends at index (train_end+input_len-1)
    train_cutoff_index = train_end + input_len - 1

    # Relative positions of test sequence starts relative to first test sequence:
    seq_rel_starts = (test_seq_indices - (train_end + val_size))  # 0..n_test-1, but we will use absolute mapping inside function
    # We'll pass the absolute start positions (relative to first possible sequence = 0)
    # For simplicity we compute forecast for each test sequence start using its *absolute* start:
    arima_preds = arima_rolling_forecast(
        series_values=series_values_unscaled,
        train_cutoff_index=train_cutoff_index,
        seq_start_positions=test_seq_indices - (0),  # absolute start positions
        input_len=input_len,
        output_len=output_len,
        order=arima_order
    )

    # Align shapes and compute ARIMA metrics
    if arima_preds is not None and arima_preds.shape[0] == y_test_unscaled.shape[0]:
        arima_rmse, arima_mae = evaluate_point_metrics(y_test_unscaled, arima_preds)
        print(f"ARIMA baseline: RMSE={arima_rmse:.4f}, MAE={arima_mae:.4f}")
    else:
        arima_rmse = arima_mae = None
        print("ARIMA baseline unavailable or shape mismatch; using LSTM as primary reference.")

    # Save model & scaler for reproducibility
    try:
        model.save(save_model_path, include_optimizer=False)
        print(f"Saved trained model to: {save_model_path}")
    except Exception as e:
        print("Model save failed:", e)

    outputs = {
        'model': model,
        'scaler': scaler,
        'history': history.history,
        'y_test': y_test_unscaled,
        'y_pred': y_pred_unscaled,
        'mc_samples': samples_unscaled,
        'interval_results': interval_results,
        'point_metrics': {'lstm_rmse': rmse, 'lstm_mae': mae, 'arima_rmse': arima_rmse, 'arima_mae': arima_mae}
    }
    return outputs

# ---------------------------
# Run script
# ---------------------------
if __name__ == "__main__":
    DATA_CSV = None  # set path to a CSV if you want to use your own dataset
    if DATA_CSV and os.path.exists(DATA_CSV):
        series = load_csv_series(DATA_CSV)
    else:
        print("Using default Sunspots dataset.")
        series = load_default_series()

    results = run_pipeline(
        series,
        input_len=36,
        output_len=12,
        test_fraction=0.2,
        val_fraction=0.1,
        units=64,
        dropout_rate=0.2,
        batch_size=32,
        epochs=100,
        mc_T=200,
        random_seed=42,
        arima_order=(5,1,0),
        save_model_path="lstm_mc_dropout_model"
    )

    print("Interval results summary:")
    for k,v in results['interval_results'].items():
        print(k, "coverage:", v['coverage'], "MIS:", v['MIS'])
